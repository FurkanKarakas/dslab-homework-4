{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 - More trains (Part III)\n",
    "\n",
    "__Hand-in:__\n",
    "\n",
    "- __Due: 12.05.2020 23:59:59 CET__\n",
    "- `git push` your final verion to your group's Renku repository before the due\n",
    "- check if `Dockerfile`, `environment.yml` and `requirements.txt` are properly written\n",
    "- add necessary comments and discussion to make your codes readable\n",
    "\n",
    "For this homework, you will be working with the real-time streams of the NS, the train company of the Netherlands. You can see an example webpage that uses the same streams to display the train information on a map: https://spoorkaart.mwnn.nl/ . \n",
    "\n",
    "To help you and avoid having too many connections to the NS streaming servers, we have setup a service that collects the streams and pushes them to our Kafka instance. The related topics are: \n",
    "\n",
    "`ndovloketnl-arrivals`: For each arrival of a train in a station, describe the previous and next station, time of arrival (planned and actual), track number,...\n",
    "\n",
    "`ndovloketnl-departures`: For each departure of a train from a station, describe the previous and next station, time of departure (planned and actual), track number,...\n",
    "\n",
    "`ndovloketnl-gps`: For each train, describe the current location, speed, bearing.\n",
    "\n",
    "The events are serialized in JSON (actually converted from XML), with properties in their original language. Google translate could help you understand all of them, but we will provide you with some useful mappings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Set up environment\n",
    "\n",
    "Run the following cells below before running the other cells of this notebook. Run them whenever you need to recreate a Spark context. Pay particular attention to your `username` settings, and make sure that it is properly set to your user name, both locally and on the remote Spark Driver.\n",
    "\n",
    "Configure your spark settings:\n",
    "1. name your spark application as `\"YOUR_GASPAR_homework_4\"`.\n",
    "2. make the required kafka jars available on the remote Spark driver.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"><b>Any application without a proper name would be promptly killed.</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure\n",
    "{\"conf\": {\n",
    "    \"spark.app.name\": <--TODO-->,\n",
    "    \"spark.jars.packages\": \"org.apache.spark:spark-streaming-kafka-0-8_2.11:2.3.1,org.apache.kafka:kafka_2.11:1.0.1\"\n",
    "}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new session unless one was already created above (check for `✔` in current session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize spark application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `username` to your GASPAR both locally and on the Spark driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import os\n",
    "username = os.environ['JUPYTERHUB_USER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%send_to_spark -i username -t str -n username"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Kafka client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pykafka import KafkaClient\n",
    "from pykafka.common import OffsetType\n",
    "\n",
    "ZOOKEEPER_QUORUM = 'iccluster044.iccluster.epfl.ch:2181,'\\\n",
    "                   'iccluster054.iccluster.epfl.ch:2181,'\\\n",
    "                   'iccluster059.iccluster.epfl.ch:2181'\n",
    "\n",
    "client = KafkaClient(zookeeper_hosts=ZOOKEEPER_QUORUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streams from Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the checkpoint folder\n",
    "checkpoint = 'hdfs:///user/{}/checkpoint/'.format(username)\n",
    "print('checkpoint created at hdfs:///user/{}/checkpoint/'.format(username))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "# Create a StreamingContext with two working thread and batch interval of 10 seconds.\n",
    "# Each time you stop a StreamingContext, you will need to recreate it.\n",
    "ssc = StreamingContext(sc, 10)\n",
    "ssc.checkpoint(checkpoint)\n",
    "\n",
    "group_id = 'ns-{0}'.format(username)\n",
    "\n",
    "# Input streams\n",
    "arrival_stream = KafkaUtils.createStream(ssc, ZOOKEEPER_QUORUM, group_id, { 'ndovloketnl-arrivals': 1})\n",
    "departure_stream = KafkaUtils.createStream(ssc, ZOOKEEPER_QUORUM, group_id, { 'ndovloketnl-departures': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, let's just print the content of the streams. Note: the output may be shown after you run `ssc.stop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arrival_stream.pprint()\n",
    "departure_stream.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination(timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to adjust the batch interval (10 seconds here) in accordance with the processing times. Use the spark UI to check if batches are not accumulating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III - Live stopping time (20 points / 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will have a look at the two other streams, namely `ndovloketnl-arrivals` and `ndovloketnl-departures`. Each time a train arrives at or leaves a station, a message is generated. Let's have a look at the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pykafka.common import OffsetType\n",
    "\n",
    "example_arrivals = client.topics[b'ndovloketnl-arrivals'].get_simple_consumer(\n",
    "    auto_offset_reset=OffsetType.EARLIEST,\n",
    "    reset_offset_on_start=True\n",
    ").consume()\n",
    "print(json.dumps(json.loads(example_arrivals.value), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example_departures = client.topics[b'ndovloketnl-departures'].get_simple_consumer(\n",
    "    auto_offset_reset=OffsetType.EARLIEST,\n",
    "    reset_offset_on_start=True\n",
    ").consume()\n",
    "print(json.dumps(json.loads(example_departures.value), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the messages have the following structure:\n",
    "\n",
    "```\n",
    "{\n",
    "  'ns1:PutReisInformatieBoodschapIn': {\n",
    "    'ns2:ReisInformatieProductDVS' or 'ns2:ReisInformatieProductDAS': {\n",
    "      'ns2:DynamischeVertrekStaat' or 'ns2:DynamischeAankomstStaat': {\n",
    "          'ns2:RitStation': <station_info>,\n",
    "          'ns2:Trein' or 'ns2:TreinAankomst': {\n",
    "              'ns2:VertrekTijd' or 'ns2:AankomstTijd': [<planned_and_actual_times>],\n",
    "              'ns2:TreinNummer': <train_number>,\n",
    "              'ns2:TreinSoort': <kind_of_train>,\n",
    "              ...\n",
    "          }\n",
    "           \n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "We can see also that the train stations have a long name `ns2:LangeNaam`, a medium name `ns2:MiddelNaam`, a short name `ns2:KorteNaam`, a station code `ns2:StationCode` and a kind of nummerical ID `ns2:UICCode`. When giving information about times, tracks, direction,... you will find sometimes the information twice with the status `Gepland` (which means planned, according to the schedule) and `Actueel`(which means the actual measured value). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question III.a. (5/20)** We want to compute the time a train stays at a station and get a real-time histogram for a given time window. To begin with, you need to write some parsing functions that will allow you to get information from the data streams. We have prepare one function `parse_train_dep` for the stream `ndovloketnl-departures`, which returns a Key-Value pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def parse_train_dep(s):\n",
    "    obj = json.loads(s)\n",
    "    tn = obj.get('ns1:PutReisInformatieBoodschapIn', {}).get('ns2:ReisInformatieProductDVS', {}).get('ns2:DynamischeVertrekStaat', {}).get('ns2:Trein', {}).get(\"ns2:TreinNummer\")\n",
    "    st = obj.get('ns1:PutReisInformatieBoodschapIn', {}).get('ns2:ReisInformatieProductDVS', {}).get('ns2:DynamischeVertrekStaat', {}).get('ns2:RitStation', {}).get(\"ns2:UICCode\")\n",
    "    if tn and st:\n",
    "        return [(\"{}-{}\".format(tn, st), obj)]\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q (1/5)__ Please check the function `parse_train_dep` above. Explain how we construct the Key and the Value, and why we construct them in this way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__: Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q (2/5)__ Take `parse_train_dep` as an example and write the function `parse_train_arr` for the stream `ndovloketnl-arrivals`. Make sure they have the same output format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parse_train_arr(s):\n",
    "    <--TODO-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q (2/5)__ Another parsing function you may need is `get_actual_time`, which will allow you to extract the actual time from the fields of time information, which are `ns2:AankomstTijd` in the arrival stream and `ns2:VertrekTijd` in the departure stream. \n",
    "\n",
    "__Note:__ These two fields may be empty and they may not contain the actual time information. In both cases the function should return `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "def get_actual_time(tab):\n",
    "    <--TODO-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the field of time in the departure stream\n",
    "example_json = json.loads(example_departures.value)\n",
    "tab = example_json.get('ns1:PutReisInformatieBoodschapIn', {}).get(\"ns2:ReisInformatieProductDVS\", {}).get(\"ns2:DynamischeVertrekStaat\", {}).get(\"ns2:Trein\", {}).get(\"ns2:VertrekTijd\",{})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime.datetime(2020, 4, 23, 15, 32)"
     ]
    }
   ],
   "source": [
    "# Example results from `get_actual_time`\n",
    "get_actual_time(tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question III.b. (5/20)** Create two Spark streams from the arrivals and departures where the records are in the form (Key, Value) using `parse_train_dep` and  `parse_train_arr`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "<--TODO-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question III.c. (5/20)** \n",
    "Every 20 seconds, we want to have a list of trains that had departed from any train station after staying for 5 minutes or less at the station. Apply a window of 20s sliding interval on arrival and departure streams. Join two streams such that trains staying for 5 minutes or less (± 20 seconds error due to sliding interval) at any station are caught by the join in the RDD window of the joined stream (you can ignore late messages). \n",
    "\n",
    "__Hint:__\n",
    "- Check [here](https://spark.apache.org/docs/2.3.1/streaming-programming-guide.html#window-operations) for windowed computations in Spark Streaming.\n",
    "- Use the methods [reduceByKeyAndWindow](https://spark.apache.org/docs/2.3.1/api/python/pyspark.streaming.html?highlight=reducebykey#pyspark.streaming.DStream.reduceByKeyAndWindow) and [join](https://spark.apache.org/docs/2.3.1/api/python/pyspark.streaming.html?highlight=reducebykey#pyspark.streaming.DStream.join) on DStream objects.\n",
    "- Both windows should have `slideDuration` of 20s\n",
    "- You have to pick the sizes of windows `windowDuration` carefully. The sizes can be different: \n",
    "    - The trains staying for 5 minutes or less (± 20 seconds error due to sliding interval) must be in the joined stream.\n",
    "    - A same stay (i.e. one train at one station) is caught in the joined stream once and only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "<--TODO-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question III.d. (5/20)** On the joined stream, compute the length of each stay (you can round to the minute) and produce a stream of histograms. You don't need to plot them, a value/count array is enough, like:\n",
    "```\n",
    "-------------------------------------------\n",
    "Time: 2018-05-17 11:10:00\n",
    "-------------------------------------------\n",
    "(0.0, 110)\n",
    "(8.0, 2)\n",
    "(4.0, 3)\n",
    "\n",
    "-------------------------------------------\n",
    "Time: 2018-05-17 11:10:20\n",
    "-------------------------------------------\n",
    "(0.0, 46)\n",
    "(4.0, 2)\n",
    "(1.0, 5)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "histograms_stream = <--TODO-->\n",
    "\n",
    "histograms_stream.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ssc.start() \n",
    "ssc.awaitTermination(timeout=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
